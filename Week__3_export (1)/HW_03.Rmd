---
title: "HW: Week 3"
author: "36-350 -- Statistical Computing"
date: "Week 3 -- Fall 2020"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Kyle Wagner   

Andrew ID: kowagner

You must submit **your own** lab as a PDF file on Gradescope.

```{r wrap-hook,echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

---

```{r linewidth=80}
trump.lines = readLines("http://www.stat.cmu.edu/~pfreeman/trump.txt")
```

---

## Question 1
*(10 points)*

Display the lines of text in `trump.lines` that contain both of the strings "America" *and* "great" (in any order, separated by any amount of text). Do so only using regexp literals.
```{r linewidth=80}
grep("great +America|America +great",trump.lines, value = TRUE)
```

## Question 2
*(10 points)*

Retrieve (but don't display) the lines of text in `trump.lines` that contain "Trump". Then break the retrieved lines into individual words (`strsplit(input," ")` to split the character vector `input` into words separated by spaces), and merge those words into a single character vector (`unlist()`!). How many unique words are there? Display the top five most commonly occurring words and how often they occur (combine `sort()` and `table()`!)
```{r linewidth=80}
trump = grep("Trump",trump.lines, value = TRUE)
res = unlist(strsplit(trump," "))
length(res) #168 unique words
sort(table(res),decreasing = TRUE)[1:5] #top five most commonly occurring words + frequency
```

## Question 3
*(10 points)*

In Q25 of Lab 3, you coded a regex to match all patterns of the following form: any letter (1 or more), then a punctuation mark, then "ve" or "ll" or "t", then a space or a punctuation mark. You called it `my.pattern`. Use `my.pattern`, along with `regexpr()` and `regmatches()`, to extract and display all the occurrences of the pattern in `trump.lines`. Then repeat the exercise using `gregexpr()` instead of `regexpr`; note that here, you'll want to `unlist()` the output from `regmatches()`. Do you get the same vector of character strings? Why or why not?
```{r linewidth=80}
my.pattern = "([a-z]|[A-Z])+[[:punct:]](ve|ll|t)( |[[:punct:]])"
reg.xpr <- regexpr(my.pattern,trump.lines)
regmatches(trump.lines,reg.xpr) 

reg.xpr1 <- gregexpr(my.pattern,trump.lines)
unlist(regmatches(trump.lines,reg.xpr1))
```
```
Do not get the same vector of character strings. This is because regxpr returns only returns the first matching substring, while gregexpr returns all matching substrings.
```

## Question 4
*(10 points)*

Come up with a strategy that splits punctuation marks or spaces, except that it keeps intact words like "I've" or "wasn't", that have a punctuation mark in the middle, in between two letters. (Or when the punctuation mark is at the beginning, as in "'em", or when there is a dollar sign at the beginning.) Apply your strategy to `trump.words` as defined below such that you display only those words with punctuation marks and/or dollar signs. (Note that I end up with 102 [not necessarily unique, but total] words when I implement this strategy. Some include '\"', which we can easily remove in a subsequent post-processing step if we so choose. Note also that a dollar sign is *not* a punctuation mark; this will affect how you define your regex. Hint: `[[:alnum:]]` is a good thing to use here.)
```{r linewidth=80}
punct_pattern = "([[:alnum:]]+[[:punct:]]{1}[[:alnum:]]+)|(\\$[[:alnum:]]+)|([[:punct:]]{1}[[:alnum:]]+)"

trump <- gregexpr(punct_pattern,trump.lines)
unlist(regmatches(trump.lines,trump))
```

---

Below, we read in lines of data from the Advanced National Seismic System (ANSS), on earthquakes of magnitude 6+, between 2002 and 2017. (You don't have to do anything yet.) 
```{r linewidth=80}
anss.lines = readLines("http://www.stat.cmu.edu/~pfreeman/anss.htm")
date.pattern = "[0-9]{4}/[0-9]{2}/[0-9]{2}"
date.lines = grep(date.pattern,anss.lines,value=TRUE)
```

---

## Question 5
*(10 points)*

Check that all the lines in `date.lines` actually start with a date, of the form YYYY/MM/DD. rather than contain a date of this form somewhere in the middle of the text. (Hint: it might help to note that you can look for non-matches, as opposed to matches, by changing one of `grep()`'s logical arguments.)
```{r linewidth=80}
date.pattern1 = "^[0-9]{4}/[0-9]{2}/[0-9]{2}" #check beginning
grep(date.pattern1,date.lines,value=TRUE,invert = TRUE) #instead of look for when true, look for when false
```

## Question 6
*(10 points)*

Which five days witnessed the most earthquakes, and how many were there, these days? Also, what happened on the day with the most earthquakes: can you find any references to this day in the news?
```{r linewidth=80}
most <- gregexpr(date.pattern,date.lines)
res <- unlist(regmatches(date.lines,most))
sort(table(res), decreasing = TRUE)[1:5]
```
```
On, 2011/03/11 the day with the most earthquakes, the 2011 TÅhoku earthquake and tsunami
occurred in Japan. The earthquake was the most powerful earthquake
ever recorded in Japan, and the 4th most powerful in the world.
```

## Question 7
*(10 points)*

Go back to the data in `date.lines`. Following steps similar to the ones you used in the lab to extract the latitude and longitude of earthquakes, extract the depth and magnitude of earthquakes. In the end, you should have one numeric vector of depths, and one numeric vector of magnitudes. Show the first three depths and the first three magnitudes. (Hint: if you use `regexpr()` and `regmatches()`, then the output from the latter will be a vector of strings. Look at this vector. The last four characters always represent the magnitudes. Use a combination of `substr()` and `as.numeric()` to create the numeric vector of magnitudes. Then use the fact that everything but the last four characters represents the depths. There are a myriad of ways to do this exercise, but this suggested way is the most concise.)
```{r linewidth=80}
depth_mag_pattern = "[0-9]{3}\\.[0-9]{2} {2}[0-9]{1}\\.[0-9]{2}"
dept_mag <- regexpr(depth_mag_pattern,date.lines)
res <- regmatches(date.lines,dept_mag)
depth = as.numeric(substr(res,1,6)) #numeric vector of depths
mag = as.numeric(substr(res,8,12)) #numeric vector of magnitudes
```

---

Here we read in text containing the fastest men's 100-meter sprint times. We retain only the lines that correspond to the sprint data, for times 9.99 seconds or better.
```{r linewidth=80}
sprint.lines = readLines("http://www.stat.cmu.edu/~pfreeman/men_100m.html")
data.lines = grep(" +(9|10)\\.",sprint.lines)
sprint.lines = sprint.lines[min(data.lines):max(data.lines)]
```

---

## Question 8
*(10 points)*

Extract the years in which the sprint times were recorded. Display them in table format. Do the same for the months. Be sure to extract the month of the sprint, not the birth month of the sprinter! (Hint: the month of the sprint is followed by a four-digit year; other instances of two digits in any given line are not. So you may have to extract more than you need, then apply `strsplit()`.)
```{r linewidth=80}
split_pattern = "[0-9]{2}\\.[0-9]{2}\\.[0-9]{4}"
reg.expr = regexpr(split_pattern,sprint.lines)
month_year = regmatches(sprint.lines,reg.expr)
res <- strsplit(unlist(month_year),split = "\\.")
res<- t(matrix(unlist(res),nrow = 3))
sort(table(res[,2]),decreasing = TRUE)
sort(table(res[,3]),decreasing = TRUE)
```

## Question 9
*(10 points)*

Extract the countries of origin (for the sprinters). Note that countries of origin are given as a capitalized three-letter abbreviation. Display the table of country origins. Display the proportion of the list that is accounted for by sprinters from the US and Jamaica.
```{r linewidth=80}
split_pattern = "[A-Z]{3}"
reg.expr = regexpr(split_pattern,sprint.lines)
country = regmatches(sprint.lines,reg.expr)
res <- sort(table(country),decreasing = TRUE)
res
US_J <- res['USA']+res['JAM']
total <- sum(res)
proportion = US_J / total
cat("\nProprtion of runners from Jamaica and United States: ",proportion)
```

## Question 10
*(10 points)*

We conclude with a web scraping exercise. I want you to go to <a href="https://arxiv.org/year/astro-ph/19">this web site</a>. On it, you see there is a set of 12 bold-faced four-digit numbers: this is the number of submitted astrophysics articles for each month of 2019. I want you to extract these numbers and place them into a single vector, with each vector element having a name: Jan for the first vector element, Feb for the second, etc. You would use `readLines()` to read in the page (pass the URL directly to the function!); this creates a vector of strings. You would then use `regexpr()` and `regmatches()` to extract the numbers (plus potentially some other stuff that you may have to pare off using `substr()`). If necessary, use "view source" to look at the html code for the web page itself to determine how best to extract the 12 numbers and nothing else. You don't want to create a table; you simply want to output the vector of four-digit numbers and add the appropriate names. (Hint: see the documentation for `Constants`. `month.abb` might be helpful here.)
```{r linewidth=80}
astro.lines = readLines("https://arxiv.org/year/astro-ph/19")
astro_pattern = "<[a-z]{1}>[0-9]{4}</[a-z]{1}> {1}\\+"
astro = regexpr(astro_pattern,astro.lines)
num_articles = regmatches(astro.lines,astro)
num_articles = as.numeric(substr(num_articles,4,7))
name.pattern = "\\([a-zA-Z]{3} {1}[0-9]{4}"
name = regexpr(name.pattern,astro.lines)
months = substr(regmatches(astro.lines,name),2,4)
names(num_articles) <- months
num_articles
```
