---
title: "HW: Week 6"
author: "36-350 -- Statistical Computing"
date: "Week 6 -- Fall 2020"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Kyle Wagner   

Andrew ID: kowagner

You must submit **your own** lab as a PDF file on Gradescope.

```{r wrap-hook,echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

---

```{r}
suppressMessages(suppressWarnings(library(tidyverse)))
```

---

## Question 1
*(10 points)*

Create a Gaussian mixture model sampler. In this sampler, a datum has a 40% chance of being sampled from a N(-1,1) distribution, and a 60% chance of being sampled from a N(2,1/9) distribution. Sample 100,000 data and create a density histogram of your result. Hint: use `sample()` with `replace` set to `TRUE` and an appropriate vector for `prob` in order to determine which of your 100,000 data should randomly be assigned to distribution 1 as opposed to distribution 2. Also note that if you create a sample of data from distribution 1 and another sample from distribution 2, you can simply combine them by doing, e.g., `x = c(sample1,sample2)`, where `x` becomes a vector of length 100,000.
```{r linewidth=80}
sample1 = rnorm(1,mean=-1,sd=1)
sample2 = rnorm(1,mean=2,sd=1/9)
samp = sample(x=c(sample1,sample2),size = 100000,replace = TRUE,prob = c(0.4,0.6))
samp.df = as.data.frame(samp)
ggplot(samp.df) +
  geom_density(aes(x = samp)) + 
  labs(
    title = "Density Plot of Sample",
    x = "Value",
    y = "Density"
  )
```

## Question 2
*(10 points)*

What is the mean of the mixture model in Q1? Compute this via importance sampling, with 100,000 sampled points. You should get an answer around 0.2 (which you can actually derive analytically: if $X \sim N(-1,1)$ and $Y \sim N(2,1/9)$, then $E[0.4X+0.6Y]) = 0.4E[X] + 0.6E[Y] = -0.4 + 1.2 = 0.8$).
```{r linewidth=80}
if ( require(extraDistr) == FALSE ) {
  install.packages("extraDistr",repos="https://cloud.r-project.org")
  library(extraDistr)
}

#calculate mean for N(-1,1)
set.seed(660)
N = 100000
x0 = rnorm(N,mean = -1,sd = 2.5) # Sample x samples from the proposal distribution h(x)
h0 = dnorm(x0,mean = -1,sd = 2.5) # Evaluate h(x)
g0 = rep(0,N)
g0 = x0 # Evaluate g(x)

f0 = function(x) {
  f.x = rep(0,length(x))
  f.x = dnorm(x,mean=-1)
  return(f.x)
}
mu0 = mean(g0*f0(x0)/h0)

#calculate mean for N(2, 1/9)

N = 100000
x1 = rnorm(N,mean = 2,sd = 3/9) # Sample x samples from the proposal distribution h(x)
h1 = dnorm(x1,mean = 2,sd = 3/9) # Evaluate h(x)
g1 = rep(0,N)
g1 = x1 # Evaluate g(x)

f1 = function(x) {
  f.x = rep(0,length(x))
  f.x = dnorm(x,mean=2,sd = 1/9)
  return(f.x)
}
mu1 = mean(g1*f1(x1)/h1)

.4*mu0 + .6*mu1
```

## Question 3
*(10 points)*

Remember the Chutes and Ladders question? (Q4 of HW 2.) Display a probability histogram that shows the empirical PDF for the number of spins, computed over 10,000 Chutes and Ladders games. Also display the average number of spins that it takes to win the game (approximately 39, give or take) and the minimum number of spins. (Display these two numbers using `cat()`, being see to indicate which is the mean and which is the minimum number of spins.) (Free feel to use your code from HW 2 as a base for what you do here.)
```{r linewidth=80}

#cat("total steps:", numSteps)

res.df = as.data.frame(rep(0,times = 10000))

#res.df

chutes = function(x){
  win = FALSE
  numSteps = 0
  position = 0
  ladder.top = c(38,14,31,42,84,44,67,91,100)
  ladder.bottom = c(1,4,9,21,28,36,51,71,80) 
  chute.top = c(98,95,93,87,64,62,56,49,47,16)
  chute.bottom = c(78,75,73,24,60,19,53,11,26,6)
  while (win == FALSE){
    move = sample(1:6,1)
    position = move + position
    w = which(ladder.bottom == position)
    if (length(w) > 0){
      position = ladder.top[w]
    }
    w = which(chute.top == position)
    if (length(w) > 0){
      position = chute.bottom[w]
    }
    if (position == 100){
      win = TRUE
    } else if (position > 100){
      position = position - move
    }
    numSteps = numSteps + 1
  }
  return(numSteps)
}
results = apply(res.df,1,chutes)
results.df = as.data.frame(results)
ggplot(results.df) + 
  geom_histogram(aes(x = results),fill = "blue") + 
  labs(
    title = "Distribution of Number of Spins to Win Chutes and Ladders",
    x = "Number of Spins",
    y = "Count"
  )

mu = mean(results.df$results)
mi = min(results.df$results)

cat("Mean:",mu,"\nMinimum:",mi)
```

## Question 4
*(10 points)*

You are given the following distribution:
$$
f(x) = \frac{4}{11}(x^3+3x+1)~~~x \in [0,1]
$$
It looks like this:
```{r linewidth=80}
x = seq(0,1,by=0.001)
fx = 4*(x^3+3*x+1)/11
ggplot(data=data.frame(x=x,y=fx),mapping=aes(x=x,y=y)) + 
  geom_line(col="peru",size=2) + ylim(0,max(fx))
```

Code up an inverse transform sampler that allows you to efficiently sample 1000 data from this distribution. Initially you will work with this and say "but...I cannot easily invert the CDF, because it's a quartic and all." To which I say "`polyroot()`, which will give you one real root between 0 and 1." To which you will say, "how do I extract that real root?" To which I will say "If you save the output of `polyroot()` as `p`, then the real roots are given by `w = which(abs(Im(p))<1.e-6`." (The 1.e-6 is a check against round-off error.) You then determine which value of `Re(p)[w]` is within the pdf bounds. Histogram your sample with the function line overlaid, and save your sample as `sample.it`. Note: pass a new argument to your histogram, `breaks=seq(0,1,by=0.05)`.
```{r linewidth=80}
res = rep(0,times = 1000)
ii = 0
while (ii <= 1000){
  x = runif(1)
  coefs = c(-1*x,4/11,6/11,0,1/11)
  p = polyroot(coefs)
  w = which(abs(Im(p))<10^(-6))
  inter = Re(p)[w]
  inter = inter[which(inter >= 0)]
  res[ii] = inter
  ii = ii + 1
}

x = seq(0,1,by=0.001)
x = x[2:1001]
fx = 4*(x^3+3*x+1)/11
res.df = data.frame(r = res,x =x,y=fx)
sample.it = res.df
ggplot(res.df) +
  geom_histogram(aes(x = res, y = ..density..),bins = 25,breaks=seq(0,1,by=0.05)) +
  geom_line(aes(x=x,y=y),col="peru",size=2) + ylim(0,max(fx))
```

## Question 5
*(10 points)*

Code up a rejection sampler that allows you to also sample 1000 data from this pdf given in the last question. Again, histogram your sample and overlay the pdf. Save your sample as `sample.rs`.
```{r linewidth=80}
set.seed(405)
k = 1000           # number of data
sample = rep(NA,k)     # vector that will hold the sampled data
ii = 1
f = function(x){
  val = (4/11)*(x^3+3*x+1)
  return(val)
}
coefs = c(-7/11,12/11,0,4/11)
m = polyroot(coefs)
m = Re(m)[1]
while ( ii <= k ) {
  sample[ii] = runif(1,min=0,max=1)            # sample along the horizontal direction
  if ( runif(1,min=0,max=20/11) <  f(sample[ii])) { # then sample along the vertical direction
    ii = ii + 1   # if the vertical sample is "below the curve", keep x[ii], increase index
  }
}

x = seq(0,1,by=0.001)
x = x[2:1001]
fx = 4*(x^3+3*x+1)/11
sample.rs = sample
sample.df = data.frame(sample.rs = sample.rs, x = x, y = fx)
ggplot(sample.df) +
  geom_histogram(aes(x = sample.rs, y = ..density..),bins=25,breaks=seq(0,1,by=0.05)) + 
  geom_line(aes(x=x,y=y),col="peru",size=2) + ylim(0,max(fx))
```

## Question 6
*(10 points)*

Test the hypothesis that `sample.it` and `sample.rs` are both sampled from the same parent population. (I mean they are, but...) Either recall how you would do a two-sample test or Google how you would do it. (Note: I'm not talking about a two-sample t-test here! We are not testing the hypothesis that the distribution means are the same. We are testing the hypothesis that both samples are drawn from the same underlying population.) There are various options for doing this; pick one, and display the p-value. If it less than 0.05, we reject the null. (Hint: it shouldn't be.) In addition, plot the empirical cdfs for both samples; see the documentation for `ecdf()` for help. (To be clear: use the base `R` function `plot()` here, and not `ggplot()`.) Note that to plot a second ecdf on top of the first, you need to call `plot()` a second time, with the argument `add=TRUE`.
```{r linewidth=80}
#mean(sample.it$r)
#mean(sample.rs)
ttest <- t.test(sample.it$r, sample.rs, alternative = "two.sided", var.equal = FALSE)
pvalue <- ttest["p.value"]
names(pvalue) <- NULL
pvalue = pvalue[[1]][1]
pvalue
Fn0 <- ecdf(sample.it$r)
Fn1 <- ecdf(sample.rs)
plot(Fn0,col = "blue",ylab = "F(x)", main = "Empirical Distribution of the Sample") +  
  plot(Fn1,col = "green", add = TRUE)
```

## Question 7
*(10 points)*

Write an inverse transform sampler that samples 10,000 data from a exponential distribution with rate parameter 1, but it keeps over the ranges $[0.5,1]$ and $[2,4]$. Make a probability histogram of your result. This time, tweak the call to `geom_histogram()` by adding the argument `breaks=seq(0,5,by=0.1)`.

This is a bit tricky. (Note: this is an inverse transform sampler, so every single randomly sampled uniform random variable has to get mapped to a valid value of $x$.) You might want to start by computing the probabilities $P[0.5 \leq X \leq 1]$ and $P[2 \leq X \leq 4]$. Call these two quantities $u_{\rm lo}$ and $u_{\rm hi}$, and sample random numbers from a Uniform(0,$u_{\rm lo}+u_{\rm hi}$) distribution. If the number is $< u_{\rm lo}$, the sampled number should be mapped to a sample from the lower range, whereas if the number is $> u_{\rm lo}$, it should be mapped to a sample from the upper range. Note: to map from you uniform random variables to exponentially distributed ones, pass your uniform r.v.'s into `qexp()`.
```{r linewidth=80}
mulo = pexp(1) - pexp(1/2)
muhi = pexp(4) - pexp(2)

u = runif(10000,max = mulo+muhi)
vals = rep(0,times = 10000)
for (ii in 1:10000){
  if (u[ii] <= mulo){
    u0 = runif(1,min=pexp(1/2),max=pexp(1))
    x = qexp(u0)
    vals[ii] = x
  } else if (u[ii] > mulo){
    u0 = runif(1,min=pexp(2),max=pexp(4))
    x = qexp(u0)
    vals[ii] = x
  }
}

exp.df = data.frame(vals)
ggplot(exp.df) +
  geom_histogram(aes(x = vals,y=..density..),breaks = seq(0,4,by=.125)) + 
  scale_x_continuous(breaks = seq(0,4,by=.5))
```

## Question 8
*(10 points)*

And now for something completely different: randomly sampling a name for your new baby, given 1930 Social Security Administration data. (You did know you were having a baby, right?)

You'll see the `babynames` tibble has five columns. Using `dplyr` techniques, extract the rows for which the year is 1930, and then create a new data frame with just the columns `name` and `prop`. `prop` is a vector which gives the proportion of children given a particular name, but for some strange reason it does not sum to 1. Anyway, use a normalized version of this vector to appropriately sample one name from the `name` column. Show that name. Done. (If you set the random number seed to 111, you should get the name "Bernard".)
```{r linewidth=80}
if ( require(babynames) == FALSE ) {
  install.packages("babynames",repos="https://cloud.r-project.org")
  library(babynames)
}

set.seed(111)

names.df <- babynames %>% filter( .$year == 1930) %>%select(name,prop)

index = sample(1:nrow(names.df),1)
name = names.df$name[index]
names.df$name[which(names.df$name == "Bernard")]
```

## Question 9
*(10 points)*

Numerically estimate the median of the pdf
$$
f(x) = \frac{2.92959}{\sqrt{2\pi}}e^{-x^2/2}~~~x \in [0,1] \,.
$$
(This is a truncated normal distribution.) The median is the value $y$ such that
$$
\int_0^y f(x) dx = 0.5 \,.
$$
A not-elegant way to do this is to use `integrate()` over and over again until you hone in on an integral value of 0.5. Don't do this. A more elegant solution is to determine, via `uniroot()`, the root of the function
$$
g(y) = \left( \int_0^y f(x) dx \right) - 0.5 \,,
$$
i.e., the value of $y$ such that $g(y) = 0$. Do do this.
```{r linewidth=80}
f = function(x){
  res = (2.92959/sqrt(2*pi)) * exp((-x^2)/2)
  return(res)
}

g = function(x){
  inte = integrate(f,0,x)$value
  return(inte - .5)
}

uniroot(g,c(0,1))$root
```

## Question 10
*(10 points)*

The ratio of the area of a circle to the area of a square into which the circle is inscribed is $\pi/4$. Does this ratio increase or decrease with dimensionality? For instance, what is the ratio of volume of a sphere to the volume of a cube into which the sphere is inscribed? Is it less than $\pi/4$? Compute (and display) the ratio for dimensions 3, 4, ..., 10. The result that you see has manifestations for, e.g., algorithms based on nearest neighbors, etc. Curse of dimensionality, n'at. (Hint: to do this calculation succinctly, consider putting samples from your uniform distribution into a $k \times d$ matrix, where $k$ is the number of sampled points, and $d$ is the dimensionality. Then you can use `apply()` to determine the distance of the points from the origin and you can easily finish the calculation from there...)
```{r linewidth=80}
dims = 3:10
k = 10000

pies = c(0,0,0,0,0,0,0,0)
for (ii in dims){
  u = runif(n=k*ii,min=-1,max=1)
  d = matrix(u,nrow=k,ncol=ii)
  f = function(x){
    c = x^2
    val = sqrt(sum(c))
    return(val)
  }
  vals = apply(d,1,f) #calculate distance
  inside = length(which(vals <= 1))
  total = k
  sample_pi = inside / total
  pies[ii-2] = sample_pi
}

cat("pi/4:", pi/4,"\n")
names(pies) <- dims
pies
```
